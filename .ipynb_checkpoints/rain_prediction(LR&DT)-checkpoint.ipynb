{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Course: Massive Data Fundamentals 502\n",
    "\n",
    "# Authors: Yuxuan Yao(yy560), Zihao Zhou(zz267), Kuiyu Zhu(kz175), Guiming Xu(gx26)\n",
    "\n",
    "# Dataset: Global Surface Summary of Day: https://registry.opendata.aws/noaa-gsod/\n",
    "\n",
    "# This python file aims at predicting rain or not by Logistic Regression and Decision Tree.\n",
    "\n",
    "# The evaluation method is Binary Classification Evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.app.name=Test SparkSession\n",
      "spark.blacklist.decommissioning.enabled=true\n",
      "spark.blacklist.decommissioning.timeout=1h\n",
      "spark.decommissioning.timeout.threshold=20\n",
      "spark.default.parallelism=16\n",
      "spark.driver.extraClassPath=/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\n",
      "spark.driver.extraJavaOptions=-XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled -XX:OnOutOfMemoryError='kill -9 %p'\n",
      "spark.driver.extraLibraryPath=/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\n",
      "spark.driver.memory=11171M\n",
      "spark.dynamicAllocation.enabled=true\n",
      "spark.emr.maximizeResourceAllocation=true\n",
      "spark.eventLog.dir=hdfs:///var/log/spark/apps\n",
      "spark.eventLog.enabled=true\n",
      "spark.executor.cores=4\n",
      "spark.executor.extraClassPath=/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar\n",
      "spark.executor.extraJavaOptions=-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=70 -XX:MaxHeapFreeRatio=70 -XX:+CMSClassUnloadingEnabled -XX:OnOutOfMemoryError='kill -9 %p'\n",
      "spark.executor.extraLibraryPath=/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native\n",
      "spark.executor.instances=2\n",
      "spark.executor.memory=10356M\n",
      "spark.files.fetchFailure.unRegisterOutputOnHost=true\n",
      "spark.hadoop.fs.s3.getObject.initialSocketTimeoutMilliseconds=2000\n",
      "spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version.emr_internal_use_only.EmrFileSystem=2\n",
      "spark.hadoop.mapreduce.fileoutputcommitter.cleanup-failures.ignored.emr_internal_use_only.EmrFileSystem=true\n",
      "spark.hadoop.yarn.timeline-service.enabled=false\n",
      "spark.history.fs.logDirectory=hdfs:///var/log/spark/apps\n",
      "spark.history.ui.port=18080\n",
      "spark.master=yarn\n",
      "spark.resourceManager.cleanupExpiredHost=true\n",
      "spark.shuffle.service.enabled=true\n",
      "spark.sql.emr.internal.extensions=com.amazonaws.emr.spark.EmrSparkSessionExtensions\n",
      "spark.sql.execution.arrow.enabled=true\n",
      "spark.sql.hive.metastore.sharedPrefixes=com.amazonaws.services.dynamodbv2\n",
      "spark.sql.parquet.fs.optimized.committer.optimization-enabled=true\n",
      "spark.sql.parquet.output.committer.class=com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter\n",
      "spark.sql.warehouse.dir=hdfs:///user/spark/warehouse\n",
      "spark.stage.attempt.ignoreOnDecommissionFetchFailure=true\n",
      "spark.submit.deployMode=client\n",
      "spark.ui.proxyBase=/proxy/application_1587493393110_0005\n",
      "spark.ui.showConsoleProgress=true\n",
      "spark.yarn.appMasterEnv.SPARK_PUBLIC_DNS=$(hostname -f)\n",
      "spark.yarn.historyServer.address=ip-172-31-36-139.ec2.internal:18080\n",
      "spark.yarn.isPython=true\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "     .appName(\"Test SparkSession\") \\\n",
    "     .getOrCreate()\n",
    "from pyspark.conf import SparkConf\n",
    "conf = SparkConf()\n",
    "print(conf.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = spark.read.option(\"header\", \"true\").csv(\"s3://aws-gsod/*/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ID: string, USAF: string, WBAN: string, Elevation: string, Country_Code: string, Latitude: string, Longitude: string, Date: string, Year: string, Month: string, Day: string, Mean_Temp: string, Mean_Temp_Count: string, Mean_Dewpoint: string, Mean_Dewpoint_Count: string, Mean_Sea_Level_Pressure: string, Mean_Sea_Level_Pressure_Count: string, Mean_Station_Pressure: string, Mean_Station_Pressure_Count: string, Mean_Visibility: string, Mean_Visibility_Count: string, Mean_Windspeed: string, Mean_Windspeed_Count: string, Max_Windspeed: string, Max_Gust: string, Max_Temp: string, Max_Temp_Quality_Flag: string, Min_Temp: string, Min_Temp_Quality_Flag: string, Precipitation: string, Precip_Flag: string, Snow_Depth: string, Fog: string, Rain_or_Drizzle: string, Snow_or_Ice: string, Hail: string, Thunder: string, Tornado: string]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140841694"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count data\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- USAF: string (nullable = true)\n",
      " |-- WBAN: string (nullable = true)\n",
      " |-- Elevation: string (nullable = true)\n",
      " |-- Country_Code: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- Day: string (nullable = true)\n",
      " |-- Mean_Temp: string (nullable = true)\n",
      " |-- Mean_Temp_Count: string (nullable = true)\n",
      " |-- Mean_Dewpoint: string (nullable = true)\n",
      " |-- Mean_Dewpoint_Count: string (nullable = true)\n",
      " |-- Mean_Sea_Level_Pressure: string (nullable = true)\n",
      " |-- Mean_Sea_Level_Pressure_Count: string (nullable = true)\n",
      " |-- Mean_Station_Pressure: string (nullable = true)\n",
      " |-- Mean_Station_Pressure_Count: string (nullable = true)\n",
      " |-- Mean_Visibility: string (nullable = true)\n",
      " |-- Mean_Visibility_Count: string (nullable = true)\n",
      " |-- Mean_Windspeed: string (nullable = true)\n",
      " |-- Mean_Windspeed_Count: string (nullable = true)\n",
      " |-- Max_Windspeed: string (nullable = true)\n",
      " |-- Max_Gust: string (nullable = true)\n",
      " |-- Max_Temp: string (nullable = true)\n",
      " |-- Max_Temp_Quality_Flag: string (nullable = true)\n",
      " |-- Min_Temp: string (nullable = true)\n",
      " |-- Min_Temp_Quality_Flag: string (nullable = true)\n",
      " |-- Precipitation: string (nullable = true)\n",
      " |-- Precip_Flag: string (nullable = true)\n",
      " |-- Snow_Depth: string (nullable = true)\n",
      " |-- Fog: string (nullable = true)\n",
      " |-- Rain_or_Drizzle: string (nullable = true)\n",
      " |-- Snow_or_Ice: string (nullable = true)\n",
      " |-- Hail: string (nullable = true)\n",
      " |-- Thunder: string (nullable = true)\n",
      " |-- Tornado: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see the clolumns we may need\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Elevation: float (nullable = true)\n",
      " |-- Latitude: float (nullable = true)\n",
      " |-- Longitude: float (nullable = true)\n",
      " |-- Month: float (nullable = true)\n",
      " |-- Mean_Temp: float (nullable = true)\n",
      " |-- Max_Temp: float (nullable = true)\n",
      " |-- Min_Temp: float (nullable = true)\n",
      " |-- Mean_Dewpoint: float (nullable = true)\n",
      " |-- Mean_Sea_Level_Pressure: float (nullable = true)\n",
      " |-- Mean_Station_Pressure: float (nullable = true)\n",
      " |-- Mean_Visibility: float (nullable = true)\n",
      " |-- Fog: float (nullable = true)\n",
      " |-- Mean_Windspeed: float (nullable = true)\n",
      " |-- Precipitation: float (nullable = true)\n",
      " |-- Rain_or_Drizzle: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data cleaning and processing\n",
    "from pyspark.sql import functions\n",
    "df = data.select(*(functions.col(col).cast(\"float\").alias(col) for col in data.columns))\n",
    "# data selection for classification\n",
    "# predict rain or drizzle\n",
    "cols = ['Elevation','Latitude', 'Longitude', 'Month', 'Mean_Temp', 'Max_Temp','Min_Temp','Mean_Dewpoint', 'Mean_Sea_Level_Pressure', \n",
    "        'Mean_Station_Pressure', 'Mean_Visibility','Fog', 'Mean_Windspeed', 'Precipitation','Rain_or_Drizzle']\n",
    "df2 = df.select(*cols)\n",
    "df2 = df2.na.drop().cache()\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Elevation', 'Latitude', 'Longitude', 'Month', 'Mean_Temp', 'Max_Temp', 'Min_Temp', 'Mean_Dewpoint', 'Mean_Sea_Level_Pressure', 'Mean_Station_Pressure', 'Mean_Visibility', 'Fog', 'Mean_Windspeed', 'Precipitation']\n"
     ]
    }
   ],
   "source": [
    "# the features without the label\n",
    "num_cols = df2.columns[:-1]\n",
    "print(num_cols)\n",
    "df2 = df2.withColumnRenamed('Rain_or_Drizzle', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stages begin\n"
     ]
    }
   ],
   "source": [
    "# logistic regression model\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "\n",
    "# stages for pipeline\n",
    "print('stages begin')\n",
    "stages = []\n",
    "assemblerInputs=num_cols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline begin\n",
      "pipeline done\n"
     ]
    }
   ],
   "source": [
    "# build pipeline\n",
    "from pyspark.ml import Pipeline\n",
    "print('pipeline begin')\n",
    "cols = df2.columns\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(df2)\n",
    "df2 = pipelineModel.transform(df2)\n",
    "selectedCols = ['features']+cols\n",
    "df2 = df2.select(selectedCols)\n",
    "train, test = df2.randomSplit([0.7, 0.3])\n",
    "print('pipeline done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Started\n",
      "Linear Regression Completed\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegression modeling\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "print('Linear Regression Started')\n",
    "LR = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=15)\n",
    "LR_model = LR.fit(train)\n",
    "print('Linear Regression Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set ROC for LR: 0.6991243570426643\n"
     ]
    }
   ],
   "source": [
    "# view the result of train dataset (AUC)\n",
    "trainingSummary = LR_model.summary\n",
    "roc = trainingSummary.roc.toPandas()\n",
    "print('Training set ROC for LR: ' + str(trainingSummary.areaUnderROC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing set Area Under ROC for Logistic Regression: 0.6993817014804423\n"
     ]
    }
   ],
   "source": [
    "# view the result of test dataset\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "predictions_LR = LR_model.transform(test)\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Testing set Area Under ROC for Logistic Regression: \" + str(evaluator.evaluate(predictions_LR, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Started\n",
      "+-----+--------------------+----------+\n",
      "|label|       rawPrediction|prediction|\n",
      "+-----+--------------------+----------+\n",
      "|  0.0|[1.7490974E7,1970...|       0.0|\n",
      "|  0.0|[1.7490974E7,1970...|       0.0|\n",
      "|  0.0|[1.7490974E7,1970...|       0.0|\n",
      "|  0.0|[1.7490974E7,1970...|       0.0|\n",
      "|  0.0|[1.7490974E7,1970...|       0.0|\n",
      "|  0.0|[1.7490974E7,1970...|       0.0|\n",
      "|  0.0|[1.7490974E7,1970...|       0.0|\n",
      "|  0.0|[1.7490974E7,1970...|       0.0|\n",
      "|  0.0|[1.7490974E7,1970...|       0.0|\n",
      "|  0.0|[1.7490974E7,1970...|       0.0|\n",
      "+-----+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Decision Tree Completed\n"
     ]
    }
   ],
   "source": [
    "# DecisionTreeClassifier modeling\n",
    "# same process as LogisticRegression modeling\n",
    "\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# train\n",
    "print('Decision Tree Started')\n",
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\n",
    "dtModel = dt.fit(train)\n",
    "predictions = dtModel.transform(test)\n",
    "predictions.select('label', 'rawPrediction', 'prediction').show(10)\n",
    "\n",
    "print('Decision Tree Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing set Area Under ROC for Decision Tree: 0.7994905015933091\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "print(\"Testing set Area Under ROC for Decision Tree: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
